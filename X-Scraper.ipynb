{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Mininng from X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review URL: https://www.amazon.com/product-reviews/B00E9M4XFI\n",
      "Scraping: https://www.amazon.com/product-reviews/B00E9M4XFI?pageNumber=1\n",
      "No reviews found. HTML snippet:\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "<!doctype html><html class=\"a-no-js\" data-19ax5a9jf=\"dingo\">\n",
      "  <head>\n",
      "<script type='text/javascript'>var ue_t0=ue_t0||+new Date();</script>\n",
      "<script type='text/javascript'>\n",
      "window.ue_ihb = (window.ue_ihb || window.ueinit || 0) + 1;\n",
      "if (window.ue_ihb === 1) {\n",
      "\n",
      "var ue_csm = window,\n",
      "    ue_hob = +new Date();\n",
      "(function(d){var e=d.ue=d.ue||{},f=Date.now||function(){return+new Date};e.d=function(b){return f()-(b?0:d.ue_t0)};e.stub=function(b,a){if(!b[a]){var c=[];b[a]=function(){c.push([c.slice.cal\n",
      "No reviews found on page 1.\n",
      "No reviews scraped.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "def extract_asin(product_url):\n",
    "    \"\"\"\n",
    "    Extract the ASIN from a standard Amazon product URL.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"/dp/([A-Z0-9]{10})\", product_url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "def get_reviews_from_page(url, headers, debug=False):\n",
    "    \"\"\"\n",
    "    Extract reviews from a single review page.\n",
    "    \"\"\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to fetch page:\", url, \"Status code:\", response.status_code)\n",
    "        return []\n",
    "    \n",
    "    html_content = response.text\n",
    "    # Check for anti-scraping or captcha page\n",
    "    if \"captcha\" in html_content.lower() or \"are you a robot\" in html_content.lower():\n",
    "        print(\"Encountered a captcha or anti-scraping page.\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    reviews = soup.find_all(\"div\", {\"data-hook\": \"review\"})\n",
    "    \n",
    "    if not reviews and debug:\n",
    "        snippet = html_content[:500]\n",
    "        print(\"No reviews found. HTML snippet:\\n\", snippet)\n",
    "    \n",
    "    review_list = []\n",
    "    for review in reviews:\n",
    "        # Extract review title\n",
    "        title_tag = review.find(\"a\", {\"data-hook\": \"review-title\"})\n",
    "        title = title_tag.text.strip() if title_tag else None\n",
    "\n",
    "        # Extract review rating (e.g., \"5.0 out of 5 stars\")\n",
    "        rating_tag = review.find(\"i\", {\"data-hook\": \"review-star-rating\"})\n",
    "        if not rating_tag:\n",
    "            rating_tag = review.find(\"i\", {\"data-hook\": \"cmps-review-star-rating\"})\n",
    "        rating = rating_tag.text.strip().split()[0] if rating_tag else None\n",
    "\n",
    "        # Extract review text\n",
    "        body_tag = review.find(\"span\", {\"data-hook\": \"review-body\"})\n",
    "        review_text = body_tag.text.strip() if body_tag else None\n",
    "\n",
    "        # Extract review date\n",
    "        date_tag = review.find(\"span\", {\"data-hook\": \"review-date\"})\n",
    "        review_date = date_tag.text.strip() if date_tag else None\n",
    "\n",
    "        # Extract reviewer name\n",
    "        reviewer_tag = review.find(\"span\", {\"class\": \"a-profile-name\"})\n",
    "        reviewer = reviewer_tag.text.strip() if reviewer_tag else None\n",
    "\n",
    "        review_list.append({\n",
    "            \"title\": title,\n",
    "            \"rating\": rating,\n",
    "            \"review_date\": review_date,\n",
    "            \"review_text\": review_text,\n",
    "            \"reviewer\": reviewer\n",
    "        })\n",
    "    return review_list\n",
    "\n",
    "def scrape_amazon_reviews(review_url, max_pages=5, debug=False):\n",
    "    \"\"\"\n",
    "    Scrape reviews from the review URL over multiple pages.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                       \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                       \"Chrome/98.0.4758.102 Safari/537.36\"),\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Connection\": \"keep-alive\"\n",
    "    }\n",
    "    all_reviews = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        page_url = f\"{review_url}?pageNumber={page}\"\n",
    "        print(\"Scraping:\", page_url)\n",
    "        reviews = get_reviews_from_page(page_url, headers, debug=debug)\n",
    "        if not reviews:\n",
    "            print(f\"No reviews found on page {page}.\")\n",
    "            break  # Stop if no reviews are found\n",
    "        all_reviews.extend(reviews)\n",
    "        time.sleep(random.uniform(2, 4))  # Polite delay between requests\n",
    "    return all_reviews\n",
    "\n",
    "def main():\n",
    "    # Use your product URL here. For example:\n",
    "    product_url = (\"https://www.amazon.com/BULKSUPPLEMENTS-COM-Creatine-Monohydrate-Powder-\"\n",
    "                   \"Unflavored/dp/B00E9M4XFI/ref=pd_hp_d_btf_rpt_sd_biaws_c_7?_encoding=UTF8&dd=gGOKekXGYApE0yMyWt5FUNNGzeC2M1AGI-JCm90QnS4%2C\"\n",
    "                   \"&ddc_refnmnt=free&pd_rd_i=B00E9M4XFI&pd_rd_w=RjVuQ&content-id=amzn1.sym.80b400d5-96d8-43db-bf9e-1b83a219e1ac&\"\n",
    "                   \"pf_rd_p=80b400d5-96d8-43db-bf9e-1b83a219e1ac&pf_rd_r=X4Z964CYR0WAXS4ARJ44&pd_rd_wg=JVvPl&\"\n",
    "                   \"pd_rd_r=741a9954-991b-45c4-a1a0-25c6c4619495&th=1\")\n",
    "    \n",
    "    # Extract ASIN from the product URL\n",
    "    asin = extract_asin(product_url)\n",
    "    if not asin:\n",
    "        print(\"ASIN not found in the URL!\")\n",
    "        return\n",
    "\n",
    "    # Build the review URL using the ASIN\n",
    "    review_url = f\"https://www.amazon.com/product-reviews/{asin}\"\n",
    "    print(\"Review URL:\", review_url)\n",
    "    \n",
    "    # Scrape reviews from the first 3 pages; set debug=True for additional output\n",
    "    reviews = scrape_amazon_reviews(review_url, max_pages=3, debug=True)\n",
    "    \n",
    "    if reviews:\n",
    "        # Create a DataFrame and display the head of the reviews\n",
    "        df = pd.DataFrame(reviews)\n",
    "        print(\"Head of scraped reviews:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Ensure the output directory exists\n",
    "        output_dir = r\"C:\\Users\\johne\\Desktop\\BRAIN\\TEMP AMZN Reviews\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_file = os.path.join(output_dir, \"reviews.csv\")\n",
    "        \n",
    "        # Save the DataFrame to CSV\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Reviews saved to {output_file}\")\n",
    "    else:\n",
    "        print(\"No reviews scraped.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review URL: https://www.amazon.com/product-reviews/B0CQMRKRV5\n",
      "Scraping: https://www.amazon.com/product-reviews/B0CQMRKRV5?pageNumber=1\n",
      "Timeout waiting for reviews on page 1: Message: \n",
      "\n",
      "No reviews scraped.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "def extract_asin(product_url):\n",
    "    match = re.search(r\"/dp/([A-Za-z0-9]{10})\", product_url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "def scrape_reviews_selenium(review_url, max_pages=3):\n",
    "    options = Options()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    # Add a realistic user-agent string\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \" \n",
    "                         \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\")\n",
    "    \n",
    "    # Replace this with your actual path to chromedriver.exe\n",
    "    service = Service(r\"C:\\WebDriver\\bin\\chromedriver.exe\")\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    \n",
    "    all_reviews = []\n",
    "    wait = WebDriverWait(driver, 15)\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{review_url}?pageNumber={page}\"\n",
    "        print(\"Scraping:\", url)\n",
    "        driver.get(url)\n",
    "        \n",
    "        try:\n",
    "            # Wait until at least one review element is present\n",
    "            wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div[data-hook='review']\")))\n",
    "        except Exception as e:\n",
    "            print(f\"Timeout waiting for reviews on page {page}: {e}\")\n",
    "            break\n",
    "        \n",
    "        reviews = driver.find_elements(By.CSS_SELECTOR, \"div[data-hook='review']\")\n",
    "        if not reviews:\n",
    "            print(f\"No reviews found on page {page}\")\n",
    "            break\n",
    "        \n",
    "        for review in reviews:\n",
    "            try:\n",
    "                title = review.find_element(By.CSS_SELECTOR, \"a[data-hook='review-title']\").text\n",
    "            except Exception:\n",
    "                title = None\n",
    "            try:\n",
    "                rating = review.find_element(By.CSS_SELECTOR, \"i[data-hook='review-star-rating']\").text.split()[0]\n",
    "            except Exception:\n",
    "                rating = None\n",
    "            try:\n",
    "                review_text = review.find_element(By.CSS_SELECTOR, \"span[data-hook='review-body']\").text\n",
    "            except Exception:\n",
    "                review_text = None\n",
    "            try:\n",
    "                review_date = review.find_element(By.CSS_SELECTOR, \"span[data-hook='review-date']\").text\n",
    "            except Exception:\n",
    "                review_date = None\n",
    "            try:\n",
    "                reviewer = review.find_element(By.CSS_SELECTOR, \"span.a-profile-name\").text\n",
    "            except Exception:\n",
    "                reviewer = None\n",
    "            \n",
    "            all_reviews.append({\n",
    "                \"title\": title,\n",
    "                \"rating\": rating,\n",
    "                \"review_date\": review_date,\n",
    "                \"review_text\": review_text,\n",
    "                \"reviewer\": reviewer\n",
    "            })\n",
    "    \n",
    "    driver.quit()\n",
    "    return all_reviews\n",
    "\n",
    "def main():\n",
    "    product_url = \"https://www.amazon.com/dp/B0CQMRKRV5\"\n",
    "    asin = extract_asin(product_url)\n",
    "    if not asin:\n",
    "        print(\"ASIN not found!\")\n",
    "        return\n",
    "    review_url = f\"https://www.amazon.com/product-reviews/{asin}\"\n",
    "    print(\"Review URL:\", review_url)\n",
    "    \n",
    "    reviews = scrape_reviews_selenium(review_url, max_pages=3)\n",
    "    \n",
    "    if reviews:\n",
    "        df = pd.DataFrame(reviews)\n",
    "        print(\"Head of scraped reviews:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        output_dir = r\"C:\\Users\\johne\\Desktop\\BRAIN\\TEMP AMZN Reviews\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_file = os.path.join(output_dir, \"reviews.csv\")\n",
    "        \n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Reviews saved to {output_file}\")\n",
    "    else:\n",
    "        print(\"No reviews scraped.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIN not found!\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "def extract_asin(product_url):\n",
    "    match = re.search(r\"/dp/([A-Za-z0-9]{10})\", product_url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "def scrape_reviews_selenium(review_url, max_pages=3):\n",
    "    options = Options()\n",
    "    # Disable headless mode for manual login\n",
    "   # options.add_argument(\"--headless\")  # Comment this out during initial testing\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \" \n",
    "                         \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\")\n",
    "    # Use your personal Chrome profile\n",
    "    options.add_argument(r\"user-data-dir=C:\\Users\\<YourUsername>\\AppData\\Local\\Google\\Chrome\\User Data\")\n",
    "    options.add_argument(\"profile-directory=Default\")  # Change if you use another profile\n",
    "    \n",
    "    service = Service(r\"C:\\WebDriver\\bin\\chromedriver.exe\")\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    wait = WebDriverWait(driver, 15)\n",
    "    \n",
    "    all_reviews = []\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{review_url}?pageNumber={page}\"\n",
    "        print(\"Scraping:\", url)\n",
    "        driver.get(url)\n",
    "        \n",
    "        try:\n",
    "            wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div[data-hook='review']\")))\n",
    "        except Exception as e:\n",
    "            print(f\"Timeout waiting for reviews on page {page}: {e}\")\n",
    "            break\n",
    "        \n",
    "        reviews = driver.find_elements(By.CSS_SELECTOR, \"div[data-hook='review']\")\n",
    "        if not reviews:\n",
    "            print(f\"No reviews found on page {page}\")\n",
    "            break\n",
    "        \n",
    "        for review in reviews:\n",
    "            try:\n",
    "                title = review.find_element(By.CSS_SELECTOR, \"a[data-hook='review-title']\").text\n",
    "            except Exception:\n",
    "                title = None\n",
    "            try:\n",
    "                rating = review.find_element(By.CSS_SELECTOR, \"i[data-hook='review-star-rating']\").text.split()[0]\n",
    "            except Exception:\n",
    "                rating = None\n",
    "            try:\n",
    "                review_text = review.find_element(By.CSS_SELECTOR, \"span[data-hook='review-body']\").text\n",
    "            except Exception:\n",
    "                review_text = None\n",
    "            try:\n",
    "                review_date = review.find_element(By.CSS_SELECTOR, \"span[data-hook='review-date']\").text\n",
    "            except Exception:\n",
    "                review_date = None\n",
    "            try:\n",
    "                reviewer = review.find_element(By.CSS_SELECTOR, \"span.a-profile-name\").text\n",
    "            except Exception:\n",
    "                reviewer = None\n",
    "            \n",
    "            all_reviews.append({\n",
    "                \"title\": title,\n",
    "                \"rating\": rating,\n",
    "                \"review_date\": review_date,\n",
    "                \"review_text\": review_text,\n",
    "                \"reviewer\": reviewer\n",
    "            })\n",
    "    \n",
    "    driver.quit()\n",
    "    return all_reviews\n",
    "\n",
    "def main():\n",
    "    product_url = \"https://www.newegg.com/p/N82E16823167063?Item=N82E16823167063\"  # Replace with your actual product URL if needed.\n",
    "    asin = extract_asin(product_url)\n",
    "    if not asin:\n",
    "        print(\"ASIN not found!\")\n",
    "        return\n",
    "    review_url = f\"https://www.newegg.com/Product/ProductReviews.aspx?Item={asin}\"\n",
    "    print(\"Review URL:\", review_url)\n",
    "    \n",
    "    reviews = scrape_reviews_selenium(review_url, max_pages=3)\n",
    "    \n",
    "    if reviews:\n",
    "        df = pd.DataFrame(reviews)\n",
    "        print(\"Head of scraped reviews:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        output_dir = r\"C:\\Users\\johne\\Desktop\\BRAIN\\TEMP AMZN Reviews\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_file = os.path.join(output_dir, \"newegg_reviews.csv\")\n",
    "        \n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Reviews saved to {output_file}\")\n",
    "    else:\n",
    "        print(\"No reviews scraped.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
